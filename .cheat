% apt

# Update content listings from package repositories
apt update

# List all available packages
apt list

# List all installed packages
apt list --installed

# Info about package (including description)
apt show -a <package-name>

# Show versions and archive areas of available package
apt list -a <package-name>

# Search in repository (packages and description)
apt search <query>

# Check updates for installed packages
apt list --upgradeable

# Update all installed packages
apt upgrade

# Upgrade all installed packages (add/remove dependencies)
apt full-upgrade

# Update specific/individual package
apt install --only-upgrade <package-name>

# Downgrade package to a specific version
apt install <package-name>=<package-version>

# Install a package from repository
apt install <package-name>

# Remove/delete package
apt remove <package-name>

# Remove/delete package (with config files)
apt purge <package-name>

# Install local dpkg package
apt install <filepath-deb>

# List dependencies of package
apt depends <package-name>

# List reverse dependencies of package
apt rdepends <package-name>

# Remove un-needed packages and dependencies
apt autoremove

% docker, containers

# List all containers (running and stopped)
docker ps -a

# List only running containers
docker ps

# Start a container
docker start <container>

# Stop a container
docker stop <container>

# Remove a container
docker rm <container>

# Remove multiple containers
docker rm <containers>

# Execute command in running container
docker exec -it <running_container> <command>

# View container logs
docker logs -f <container>

# Inspect container details
docker inspect <container>

# Copy files from container to host
docker cp <container>:<container_path> <host_path>

# Copy files from host to container
docker cp <host_path> <container>:<container_path>

$ container: docker ps -a --format "table {{.Names}}\t{{.Status}}" --- --column 1 --header-lines 1
$ running_container: docker ps --format "table {{.Names}}\t{{.Status}}" --- --column 1 --header-lines 1
$ containers: docker ps -a --format "{{.Names}}" --- --multi --expand
$ command: echo "bash sh /bin/bash /bin/sh" | tr ' ' '\n'
$ container_path: echo "/app /var/log /etc /home /tmp" | tr ' ' '\n'
$ host_path: echo "$(pwd) /tmp /home/$USER" | tr ' ' '\n'

% docker, images

# List all images
docker images

# Pull an image from registry
docker pull <image_name>

# Remove an image
docker rmi <image>

# Remove multiple images
docker rmi <images>

# Build image from Dockerfile
docker build -t <tag_name> <build_context>

# Tag an image
docker tag <source_image> <target_image>

# Push image to registry
docker push <image_name>

# Show image history
docker history <image>

# Remove dangling images
docker image prune

# Remove all unused images
docker image prune -a

$ image: docker images --format "table {{.Repository}}:{{.Tag}}\t{{.Size}}" --- --column 1 --header-lines 1
$ images: docker images --format "{{.Repository}}:{{.Tag}}" --- --multi --expand
$ image_name: echo "nginx alpine ubuntu node redis mysql postgres" | tr ' ' '\n'
$ tag_name: echo "myapp:latest myapp:v1.0 myapp:dev" | tr ' ' '\n'
$ build_context: echo ". ./docker /path/to/dockerfile" | tr ' ' '\n'
$ source_image: docker images --format "{{.Repository}}:{{.Tag}}"
$ target_image: echo "myregistry/myapp:latest localhost:5000/myapp:v1.0" | tr ' ' '\n'

% docker, networks

# List all networks
docker network ls

# Create a network
docker network create <network_name>

# Remove a network
docker network rm <network>

# Connect container to network
docker network connect <network> <container>

# Disconnect container from network
docker network disconnect <network> <container>

# Inspect network details
docker network inspect <network>

$ network: docker network ls --format "{{.Name}}" --- --header-lines 1
$ network_name: echo "myapp-network frontend-network backend-network" | tr ' ' '\n'

% docker, volumes

# List all volumes
docker volume ls

# Create a volume
docker volume create <volume_name>

# Remove a volume
docker volume rm <volume>

# Remove unused volumes
docker volume prune

# Inspect volume details
docker volume inspect <volume>

$ volume: docker volume ls --format "{{.Name}}"
$ volume_name: echo "db-data logs-data config-data" | tr ' ' '\n'

% docker, compose

# Start all services
docker compose up -d

# Stop all services
docker compose down

# Restart specific service
docker compose restart <service>

# View logs of all services
docker compose logs -f

# View logs of specific service
docker compose logs -f <service>

# Scale a service
docker compose up -d --scale <service>=<replicas>

# Build and start services
docker compose up --build -d

# Execute command in service container
docker compose exec <service> <command>

$ service: docker-compose config --services 2>/dev/null || echo "web db redis nginx app api worker" | tr ' ' '\n'
$ replicas: echo "1 2 3 5 10" | tr ' ' '\n'

% docker, system

# Show system information
docker system info

# Show disk usage
docker system df

# Clean up unused resources
docker system prune

# Clean up everything (including volumes)
docker system prune -a --volumes

# Show running processes in containers
docker stats

# Monitor container resource usage
docker stats <container>

% npm, node, js

# initial new package
npm init

# initial immediately a new package
npm init -y

# install all dependencies packages
npm install

# install all dependencies packages for specific folder
npm install --prefix <folder>

# install all dev dependencies packages
npm install --save-dev

# install all dev dependencies for a specific folder
npm install --save-dev --prefix <folder>

# install a specified package
npm install <package_name>

# install a specified dev package
npm install <package_name> --save-dev

# install globally a specified package
npm install <package_name> -g

# install for a specific folder
npm install <package_name> --prefix <folder>

# install a specified dev package for a specific folder
npm install <package_name> --save-dev --prefix <folder>

# run a script
npm run <script>

# run a script for a specific folder
npm run <script> --prefix <folder>

$ script: node -p "Object.keys(require('./package.json').scripts).join('\n')"

% yarn, node, js

# initial new package
yarn init

# install all dependencies packages
yarn install

# install all dev dependencies packages
yarn install --save-dev

# install a specified package
yarn add <package_name>

# install a specified dev package
yarn add <package_name> --dev

# install globally a specified package
yarn global add <package_name>

# run a script
yarn run <script>

$ script: node -p "Object.keys(require('./package.json').scripts).join('\n')"

% nvm, node, js

# install a specified version of node
nvm install <version>

# list available versions
nvm ls-remote

# use installed node's version
nvm use <version>

# set a node's version as default
nvm alias default <version>

% networking, monitoring

# Show network interfaces
ip addr show

# Show routing table
ip route show

# Test connectivity to host
ping -c 4 <hostname>

# Test port connectivity
telnet <hostname> <port>

# Scan ports on host
nmap -p <port_range> <hostname>

# Check listening ports
netstat -tlnp

# Show network connections
ss -tuln

# Monitor network traffic
tcpdump -i <interface> host <hostname>

# Show DNS resolution
nslookup <hostname>

# Trace route to destination
traceroute <hostname>

$ hostname: echo "localhost google.com 8.8.8.8 github.com" | tr ' ' '\n'
$ port: echo "22 80 443 8080 3306 5432 6379 25565" | tr ' ' '\n'
$ port_range: echo "1-1000 22,80,443 1-65535" | tr ' ' '\n'
$ interface: ip link show | grep -E '^[0-9]+:' | awk '{print $2}' | tr -d ':'

% git, devops

# Set global git user name
git config --global user.name <name>

# Set global git user email
git config --global user.email <email>

# Initializes a git repository
git init

# Clone a git repository
git clone -b <branch_name> <repository> <clone_directory>

# Shallow clone with depth 1 with all branches and submodules
git clone --depth=1 --no-single-branch --recurse-submodules <repository> <clone_directory>

# Rebase upstream master into local/origin master (use if people don't clone your repository)
git fetch <remote_name>
git checkout master
git rebase <remote_name>/master
git fetch --unshallow origin
git push -f origin master

# Merge upstream master into local/origin master (use if people clone your repository)
git fetch <remote_name>
git checkout master
git merge <remote_name>/master
git fetch --unshallow origin
git push -f origin master

# View all available remote for a git repository
git remote --verbose

# Adds a remote for a git repository
git remote add <remote_name> <remote_url>

# Renames a remote for a git repository
git remote rename <old_remote_name> <new_remote_name>

# Remove a remote for a git repository
git remote remove <remote_name>

# Checkout to branch
git checkout <branch>

# Displays the current status of a git repository
git status

# Displays unstaged changes for file
cd <toplevel_directory>; \
    git diff <unstaged_files>

# Stage single or multiple files
cd <toplevel_directory>; \
    git add <changed_files>;

# Stage all files in project
git add -A

# Create commit for staged files
git commit -m "<commit_description>"

# Create backdated commit for staged files
git commit --date="<number_of_days_ago> days ago" -m "<commit_description>"

# Pushes committed changes to remote repository
git push -u <remote_name> <branch_name>

# Pushes changes to a remote repository overwriting another branch
git push <remote_name> <branch>:<branch_to_overwrite>

# Overwrites remote branch with local branch changes
git push <remote_name> <branch_name> -f

# Pulls changes to a remote repo to the local repo
git pull --ff-only

# Merges changes on one branch into current branch
git merge <branch_name>

# Abort the current conflict resolution process, and try to reconstruct the pre-merge state.
git merge --abort

# Displays log of commits for a repo
git log

# Displays formatted log of commits for a repo
git log --all --decorate --oneline --graph

# Clear everything
git clean -dxf

# Sign all commits in a branch based on master
git rebase master -S -f

# See all open pull requests of a user on Github
navi fn url::open 'https://github.com/pulls?&q=author:<user>+is:open+is:pr'

# Checkout a branch from a fork
git fetch origin pull/<pr_number>/head:pr/<pr_number> \
   && git checkout pr/<pr_number>

# Add a new module
git submodule add <repository> <path>

# Update module
git submodule update --init

# Update module without init
git submodule update

# Pull all submodules
git submodule foreach git pull origin master

# Update all submodules
git submodule update --init --recursive

# Skip git hooks
git commit --no-verify

# Create new branch from current HEAD
git checkout -b <new_branch_name>

# Remove commits from local repository (destroy changes)
git reset --hard HEAD~<number_of_commits>

# Remove commits from local repository (keep changes)
git reset --soft HEAD~<number_of_commits>

$ branch: git branch | awk '{print $NF}'
$ toplevel_directory: git rev-parse --show-toplevel
$ unstaged_files: git status --untracked-files=no -s --porcelain | awk '{print $NF}' --- --multi true
$ changed_files: git status --untracked-files=all -s --porcelain | awk '{print $NF}' --- --multi true

% gpg

# gpg version
gpg --version

# gpg generate key
gpg --gen-key

# list keys
gpg --list-keys

# distribute public key to key server
gpg --keyserver <key_server> --send-keys <public_key>

# export public key
gpg --output <filename_gpg> --export <key_name>

# import public key
gpg --import <filename_gpg>

# encrypt document
gpg --output <output_filename_gpg> --encrypt --recipient <public_key> <input_filename>

# decrypt document
gpg --output <filename> --decrypt <filename_gpg>

# make a signature
gpg --output <filename_sig> --sign <filename>

# verify signature
gpg --output <filename> <filename> --decrypt <filename_sig>

# clearsign documents
gpg --clearsign <filename>

# detach signature
gpg --output <filename_sig> --detach-sig <filename>

% backup, sync

# Backup directory with rsync
rsync -av <source_dir>/ <destination_dir>/

# Backup to remote server
rsync -av -e ssh <source_dir>/ <user>@<remote_host>:<remote_dir>/

# Create tar archive
tar -czf <archive_name>.tar.gz <directory>

# Extract tar archive
tar -xzf <archive_file>

# Compress directory with zip
zip -r <archive_name>.zip <directory>

# Extract zip file
unzip <zip_file>

$ source_dir: echo "/home/$USER /var/www /opt /etc" | tr ' ' '\n'
$ destination_dir: echo "/backup /tmp/backup /mnt/backup" | tr ' ' '\n'
$ user: echo "$USER root admin deploy" | tr ' ' '\n'
$ remote_host: echo "backup.example.com 192.168.1.100 server.local" | tr ' ' '\n'
$ remote_dir: echo "/backup /home/backup /var/backups" | tr ' ' '\n'
$ archive_name: echo "backup-$(date +%Y%m%d) myapp-backup project-backup" | tr ' ' '\n'
$ archive_file: find . -name "*.tar.gz" -o -name "*.tgz" | head -10
$ zip_file: find . -name "*.zip" | head -10

% ssl, security

# Generate SSL certificate with openssl
openssl req -x509 -newkey rsa:4096 -keyout <key_file> -out <cert_file> -days <days>

# Check SSL certificate expiration
openssl x509 -in <cert_file> -noout -dates

# Test SSL connection
openssl s_client -connect <hostname>:443

# Generate SSH key pair
ssh-keygen -t rsa -b 4096 -C "<email>"

# Copy SSH public key to remote server
ssh-copy-id <user>@<hostname>

# Check file permissions
ls -la <file_path>

# Change file permissions
chmod <permissions> <file_path>

# Change file ownership
chown <user>:<group> <file_path>

$ key_file: echo "server.key app.key private.key" | tr ' ' '\n'
$ cert_file: echo "server.crt app.crt certificate.pem" | tr ' ' '\n' 
$ days: echo "365 730 1095 30 90" | tr ' ' '\n'
$ email: echo "$USER@$(hostname) admin@example.com dev@company.com" | tr ' ' '\n'
$ file_path: find . -maxdepth 2 -type f | head -10
$ permissions: echo "644 755 600 700 777" | tr ' ' '\n'
$ group: echo "$(groups | tr ' ' '\n')"

% database, management

# Connect to MySQL database
mysql -u <db_user> -p<password> -h <db_host> <database>

# Backup MySQL database
mysqldump -u <db_user> -p<password> <database> > <backup_file>.sql

# Restore MySQL database
mysql -u <db_user> -p<password> <database> < <backup_file>.sql

# Connect to PostgreSQL database
psql -U <db_user> -h <db_host> -d <database>

# Backup PostgreSQL database
pg_dump -U <db_user> -h <db_host> <database> > <backup_file>.sql

# Connect to Redis
redis-cli -h <db_host> -p <redis_port>

# Connect to MongoDB
mongo --host <db_host>:<mongo_port> <database>

$ db_user: echo "root admin user app_user postgres" | tr ' ' '\n'
$ password: echo "password admin 123456 secret" | tr ' ' '\n'
$ db_host: echo "localhost 127.0.0.1 db.example.com 192.168.1.100" | tr ' ' '\n'
$ database: echo "myapp production development test wordpress" | tr ' ' '\n'
$ backup_file: echo "backup-$(date +%Y%m%d) db-backup myapp-backup" | tr ' ' '\n'
$ redis_port: echo "6379 6380 16379" | tr ' ' '\n'
$ mongo_port: echo "27017 27018 27019" | tr ' ' '\n'

% database, dump-restore

# MySQL full backup with all databases
mysqldump -u <db_user> -p<password> --all-databases > <all_db_backup>.sql

# MySQL backup with structure only
mysqldump -u <db_user> -p<password> --no-data <database> > <structure_backup>.sql

# MySQL backup with data only
mysqldump -u <db_user> -p<password> --no-create-info <database> > <data_backup>.sql

# MySQL backup specific tables
mysqldump -u <db_user> -p<password> <database> <tables> > <table_backup>.sql

# MySQL backup with compression
mysqldump -u <db_user> -p<password> <database> | gzip > <compressed_backup>.sql.gz

# PostgreSQL full backup
pg_dumpall -U <db_user> -h <db_host> > <pg_full_backup>.sql

# PostgreSQL backup with custom format
pg_dump -U <db_user> -h <db_host> -F c -f <custom_backup>.dump <database>

# PostgreSQL backup specific schema
pg_dump -U <db_user> -h <db_host> -n <schema> <database> > <schema_backup>.sql

# PostgreSQL restore from custom format
pg_restore -U <db_user> -h <db_host> -d <database> <custom_backup>.dump

# MongoDB backup
mongodump --host <db_host>:<mongo_port> --db <database> --out <mongo_backup_dir>

# MongoDB restore
mongorestore --host <db_host>:<mongo_port> --db <database> <mongo_backup_dir>/<database>

# MongoDB backup with authentication
mongodump --host <db_host>:<mongo_port> -u <db_user> -p <password> --db <database> --out <mongo_backup_dir>

# Redis backup (save current state)
redis-cli -h <db_host> -p <redis_port> BGSAVE

# Redis export specific database
redis-cli -h <db_host> -p <redis_port> -n <redis_db> --rdb <redis_backup>.rdb

$ all_db_backup: echo "all_databases_$(date +%Y%m%d) full_backup mysql_all_dbs" | tr ' ' '\n'
$ structure_backup: echo "structure_$(date +%Y%m%d) schema_backup db_structure" | tr ' ' '\n'
$ data_backup: echo "data_$(date +%Y%m%d) data_backup db_data" | tr ' ' '\n'
$ tables: echo "users products orders customers invoices logs" | tr ' ' '\n'
$ table_backup: echo "tables_$(date +%Y%m%d) specific_tables user_data" | tr ' ' '\n'
$ compressed_backup: echo "backup_$(date +%Y%m%d) compressed_db mysql_backup" | tr ' ' '\n'
$ pg_full_backup: echo "postgres_full_$(date +%Y%m%d) pg_all_databases complete_backup" | tr ' ' '\n'
$ custom_backup: echo "custom_$(date +%Y%m%d) postgres_backup db_custom" | tr ' ' '\n'
$ schema: echo "public schema1 app_schema user_schema" | tr ' ' '\n'
$ schema_backup: echo "schema_$(date +%Y%m%d) public_schema app_schema" | tr ' ' '\n'
$ mongo_backup_dir: echo "./mongo_backup /backup/mongodb /tmp/mongo_backup" | tr ' ' '\n'
$ redis_backup: echo "redis_$(date +%Y%m%d) redis_dump backup" | tr ' ' '\n'
$ redis_db: echo "0 1 2 3 15" | tr ' ' '\n'

% database, migration

# MySQL to MySQL migration
mysqldump -u <source_user> -p<source_password> -h <source_host> <source_db> | mysql -u <dest_user> -p<dest_password> -h <dest_host> <dest_db>

# PostgreSQL to PostgreSQL migration
pg_dump -U <source_user> -h <source_host> <source_db> | psql -U <dest_user> -h <dest_host> <dest_db>

# CSV import to MySQL
mysql -u <db_user> -p<password> -e "LOAD DATA LOCAL INFILE '<csv_file>' INTO TABLE <table_name> FIELDS TERMINATED BY ',' ENCLOSED BY '\"' LINES TERMINATED BY '\n' IGNORE 1 ROWS;" <database>

# CSV export from MySQL
mysql -u <db_user> -p<password> -e "SELECT * FROM <table_name> INTO OUTFILE '<output_csv>' FIELDS TERMINATED BY ',' ENCLOSED BY '\"' LINES TERMINATED BY '\n';" <database>

# PostgreSQL CSV import
psql -U <db_user> -h <db_host> -d <database> -c "\copy <table_name> FROM '<csv_file>' WITH CSV HEADER"

# PostgreSQL CSV export
psql -U <db_user> -h <db_host> -d <database> -c "\copy <table_name> TO '<output_csv>' WITH CSV HEADER"

# SQLite backup
sqlite3 <sqlite_db> ".backup <backup_db>"

# SQLite restore
sqlite3 <sqlite_db> ".restore <backup_db>"

$ source_user: echo "root admin backup_user source_db_user" | tr ' ' '\n'
$ source_password: echo "password admin 123456 backup_pass" | tr ' ' '\n'
$ source_host: echo "source.db.com 192.168.1.100 old-server.local" | tr ' ' '\n'
$ source_db: echo "old_database source_db legacy_system" | tr ' ' '\n'
$ dest_user: echo "root admin target_user dest_db_user" | tr ' ' '\n'
$ dest_password: echo "password admin 123456 target_pass" | tr ' ' '\n'
$ dest_host: echo "target.db.com 192.168.1.200 new-server.local" | tr ' ' '\n'
$ dest_db: echo "new_database target_db production_system" | tr ' ' '\n'
$ csv_file: find . -name "*.csv" -type f | head -10
$ table_name: echo "users products orders customers invoices employees" | tr ' ' '\n'
$ output_csv: echo "export_$(date +%Y%m%d).csv data_export.csv table_data.csv" | tr ' ' '\n'
$ sqlite_db: find . -name "*.db" -o -name "*.sqlite" -o -name "*.sqlite3" -type f | head -10
$ backup_db: echo "backup_$(date +%Y%m%d).db sqlite_backup.db database_backup.db" | tr ' ' '\n'

% security, scanning

# Basic port scan
nmap <target>

# Scan specific ports
nmap -p <ports> <target>

# TCP SYN scan (stealth scan)
nmap -sS <target>

# UDP scan
nmap -sU <target>

# Service version detection
nmap -sV <target>

# OS detection
nmap -O <target>

# Aggressive scan (OS, version, scripts, traceroute)
nmap -A <target>

# Scan with NSE scripts
nmap --script <script_category> <target>

# Fast scan (top 100 ports)
nmap -F <target>

# Scan network range
nmap <network_range>

# Banner grabbing with netcat
nc -nv <target> <port>

# SSL/TLS cipher scan
nmap --script ssl-enum-ciphers -p 443 <target>

# HTTP methods enumeration
nmap --script http-methods <target>

# Directory brute force with gobuster
gobuster dir -u http://<target> -w <wordlist>

# Web vulnerability scan with nikto
nikto -h <target>

$ target: echo "127.0.0.1 192.168.1.1 scanme.nmap.org example.com" | tr ' ' '\n'
$ ports: echo "22,80,443 1-1000 21,22,23,25,53,80,110,143,443,993,995 3389,5432,3306,6379" | tr ' ' '\n'
$ script_category: echo "vuln auth brute discovery safe default" | tr ' ' '\n'
$ network_range: echo "192.168.1.0/24 10.0.0.0/8 172.16.0.0/12 192.168.0.1-254" | tr ' ' '\n'
$ wordlist: echo "/usr/share/wordlists/dirb/common.txt /usr/share/wordlists/dirbuster/directory-list-2.3-medium.txt" | tr ' ' '\n'

% security, vulnerability, assessment

# Nmap vulnerability scan
nmap --script vuln <target>

# Check for SMB vulnerabilities
nmap --script smb-vuln* <target>

# Scan for web vulnerabilities
nmap --script http-vuln* <target>

# Check for SSL vulnerabilities
nmap --script ssl-* <target>

# DNS enumeration
nmap --script dns-brute <domain>

# SNMP enumeration
nmap -sU --script snmp-brute <target>

# SMB enumeration
nmap --script smb-enum* <target>

# FTP anonymous login check
nmap --script ftp-anon <target>

# HTTP title and server info
nmap --script http-title,http-server-header <target>

# Heartbleed vulnerability check
nmap --script ssl-heartbleed <target>

# Shellshock vulnerability check
nmap --script http-shellshock <target>

# Check for default credentials
nmap --script auth <target>

$ domain: echo "example.com google.com github.com" | tr ' ' '\n'

% penetration, testing

# TCP connect scan (full connection)
nmap -sT <target>

# Xmas scan (firewall evasion)
nmap -sX <target>

# FIN scan (firewall evasion)
nmap -sF <target>

# NULL scan (firewall evasion)
nmap -sN <target>

# Decoy scan (hide source IP)
nmap -D <decoy_ips> <target>

# Zombie scan (idle scan)
nmap -sI <zombie_host> <target>

# Timing template (0-5, paranoid to insane)
nmap -T<timing> <target>

# Fragment packets
nmap -f <target>

# Spoof source port
nmap --source-port <sport> <target>

# Randomize target order
nmap --randomize-hosts <targets>

# Save results to file
nmap -oN <output_file> <target>

# Save results in XML format
nmap -oX <xml_file> <target>

$ decoy_ips: echo "192.168.1.100,192.168.1.200,RND 10.0.0.1,10.0.0.2,ME" | tr ' ' '\n'
$ zombie_host: echo "192.168.1.50 10.0.0.100" | tr ' ' '\n'
$ timing: echo "0 1 2 3 4 5" | tr ' ' '\n'
$ sport: echo "53 80 443 8080" | tr ' ' '\n'
$ targets: echo "192.168.1.1-50 scanme.nmap.org example.com" | tr ' ' '\n'
$ output_file: echo "scan_results.txt nmap_scan_$(date +%Y%m%d).txt" | tr ' ' '\n'
$ xml_file: echo "scan_results.xml nmap_scan_$(date +%Y%m%d).xml" | tr ' ' '\n'

% performance, load-testing

# Basic HTTP load test with wrk
wrk -t<threads> -c<connections> -d<duration> <url>

# Load test with custom script
wrk -t<threads> -c<connections> -d<duration> -s <script_file> <url>

# Load test with POST data
wrk -t<threads> -c<connections> -d<duration> -H "Content-Type: application/json" -s <post_script> <url>

# Generate wrk POST script
echo 'wrk.method = "POST"; wrk.body = "<json_payload>"; wrk.headers["Content-Type"] = "application/json"' > <post_script>

# Load test with custom headers
wrk -t<threads> -c<connections> -d<duration> -H "<header_name>: <header_value>" <url>

# Benchmark with Apache Bench (ab)
ab -n <requests> -c <concurrency> <url>

# Benchmark with custom headers (ab)
ab -n <requests> -c <concurrency> -H "<header_name>: <header_value>" <url>

# Benchmark POST request (ab)
ab -n <requests> -c <concurrency> -p <post_data_file> -T <content_type> <url>

# Siege load testing
siege -c<users> -t<time> <url>

# Siege with URL list
siege -c<users> -t<time> -f <url_list>

# Monitor system during load test
iostat -x 1

# Monitor network during load test  
iftop -i <interface>

$ threads: echo "1 2 4 8 12 16" | tr ' ' '\n'
$ connections: echo "10 50 100 200 500 1000" | tr ' ' '\n'
$ duration: echo "10s 30s 1m 2m 5m 10m" | tr ' ' '\n'
$ url: echo "http://localhost:8080 http://localhost:3000 https://example.com http://192.168.1.100" | tr ' ' '\n'
$ script_file: echo "post.lua get.lua auth.lua benchmark.lua" | tr ' ' '\n'
$ post_script: echo "post.lua json-post.lua api-test.lua" | tr ' ' '\n'
$ json_payload: echo '{"test": "data"}' '{"user": "admin", "pass": "secret"}' '{"action": "benchmark"}' | tr ' ' '\n'
$ header_name: echo "Authorization Content-Type User-Agent X-API-Key" | tr ' ' '\n'
$ header_value: echo "Bearer token123 application/json curl/7.68.0 secret-key" | tr ' ' '\n'
$ requests: echo "100 1000 5000 10000 50000" | tr ' ' '\n'
$ concurrency: echo "1 5 10 20 50 100" | tr ' ' '\n'
$ post_data_file: echo "post.json post.txt api-data.json" | tr ' ' '\n'
$ content_type: echo "application/json application/x-www-form-urlencoded text/plain" | tr ' ' '\n'
$ users: echo "10 25 50 100 200" | tr ' ' '\n'
$ time: echo "1m 2m 5m 10m 30m" | tr ' ' '\n'
$ url_list: echo "urls.txt endpoints.txt api-urls.txt" | tr ' ' '\n'

% system, monitoring

# Show system resource usage
top

# Show disk usage
df -h

# Show directory sizes
du -sh <directory>/*

# Show memory usage
free -h

# Show running processes
ps aux

# Find process by name
pgrep -f <process_name>

# Kill process by PID
kill -9 <pid>

# Monitor file changes
tail -f <log_file>

# Show system uptime
uptime

# Show logged in users
who

$ directory: echo "/ /var/log /home /tmp /opt" | tr ' ' '\n'
$ process_name: echo "nginx docker mysql postgres redis node python" | tr ' ' '\n'
$ pid: ps aux | awk 'NR>1 {print $2 "\t" $11}' | head -20 --- --column 1 --header-lines 1
$ log_file: find /var/log -name "*.log" -type f 2>/dev/null | head -10

% performance, monitoring

# Real-time system performance
htop

# Monitor disk I/O
iotop

# Monitor network bandwidth
nethogs

# System performance summary
vmstat 1

# Memory usage details
pmap -x <pid>

# Process file descriptors
lsof -p <pid>

# Network connections by process
lsof -i -P -n

# Monitor specific port
lsof -i :<port>

# Check process CPU usage
pidstat -u 1

# Check process I/O usage  
pidstat -d 1

# Monitor MySQL processes
mysqladmin -u <db_user> -p<password> processlist

# Monitor PostgreSQL activity
psql -U <db_user> -h <db_host> -d <database> -c "SELECT * FROM pg_stat_activity;"

# Redis monitoring
redis-cli -h <db_host> -p <redis_port> monitor

# MongoDB performance stats
mongo --host <db_host>:<mongo_port> --eval "db.runCommand({serverStatus:1})"

# Apache/Nginx access log analysis
tail -f <access_log> | grep -E "(GET|POST|PUT|DELETE)"

$ access_log: echo "/var/log/apache2/access.log /var/log/nginx/access.log /var/log/httpd/access_log" | tr ' ' '\n'

% shell, apps

# Start appliction
xdg-open <programme> 

# Open finder with current folder
open .

% shell, variables

# Register variable
export <TESTING>=<Variable-text>

# Echo variable
echo $<Variable>

# Unset variable
unset <Variable>

% systemctl, service

# Start service
systemctl start <service_inactive>

# Stop service
systemctl stop <service_active>

# Enable service
systemctl enable <service_disabled>

# Disable service
systemctl disable <service_enabled>

# Restart service
systemctl restart <service>

# Reload service
systemctl reload <service_active>

# Service status
systemctl status <service>

# List running services
systemctl list-units --type=service --state=running

# List enabled services
systemctl list-unit-files --type=service --state=enabled

# List disabled services
systemctl list-unit-files --type=service --state=disabled

$ service_inactive: systemctl list-units --type=service --state=inactive | awk '{print $1}' | grep .service | sed 's/.service$//'
$ service_active: systemctl list-units --type=service --state=active | awk '{print $1}' | grep .service | sed 's/.service$//'
$ service_enabled: systemctl list-unit-files --type=service --state=enabled | awk '{print $1}' | grep .service | sed 's/.service$//'
$ service_disabled: systemctl list-unit-files --type=service --state=disabled | awk '{print $1}' | grep .service | sed 's/.service$//'
$ service: systemctl list-units --type=service --all | awk '{print $1}' | grep .service | sed 's/.service$//'

% weather

# Show weather info for current location
curl -s "wttr.in" \
   | grep -v "New feature" \
   | grep -v Follow

# Show weather info for a specific location
curl -s "wttr.in/<location>" \
   | grep -v "New feature" \
   | grep -v Follow